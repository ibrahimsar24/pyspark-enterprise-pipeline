# ğŸ› ï¸ PySpark ETL Pipeline â€“ Enterprise-Scale Log Processing

This project simulates a real-world **data engineering pipeline** using PySpark. It processes 100,000+ nested JSON logs into flattened, partitioned Parquet files â€” followed by **SQL-based analytics and indexing simulation**.

> ğŸ“Œ Built to match job roles like **Data Engineer** or **Software Engineer III** in data-heavy environments (e.g. JPMorgan Chase).

---

## ğŸš€ Features

- âœ… Ingests large-scale nested JSON log data
- âœ… Flattens complex structures (nested fields, arrays)
- âœ… Writes partitioned Parquet files (`event_type`)
- âœ… Simulates analytics with PySpark SQL
- âœ… Simulates indexing with multi-column aggregation

---

## ğŸ“‚ Folder Structure

```
pyspark-etl/
â”œâ”€â”€ data/
â”‚   â””â”€â”€ large_input_logs.json      # 100,000 synthetic logs
â”œâ”€â”€ output/                        # Parquet files (created after ETL)
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ etl.py                     # Main pipeline logic
â”‚   â”œâ”€â”€ transformer.py            # Flattens nested JSON
â”‚   â””â”€â”€ writer.py                 # Writes partitioned parquet
â”œâ”€â”€ main.py                        # Entry point to run ETL
â”œâ”€â”€ simulate_sql.py               # SQL & indexing simulation
â”œâ”€â”€ requirements.txt              # Python dependencies
â””â”€â”€ README.md                     # Youâ€™re reading it!
```

---

## ğŸ§ª Input Sample

```json
{
  "event_id": "abc123",
  "timestamp": "2025-05-01T10:15:00",
  "user": {
    "id": "user001",
    "location": "New York"
  },
  "event_type": "login",
  "details": {
    "ip": "192.168.0.1",
    "device": "mobile"
  }
}
```

---

## âš™ï¸ How to Run

### 1. Install Dependencies

> You can use GitHub Codespaces or a local Python 3.10+ environment.

```bash
pip install -r requirements.txt
```

### 2. Run the ETL Pipeline

```bash
python main.py
```

This will:
- Load `data/large_input_logs.json`
- Flatten nested fields
- Write partitioned Parquet files to `output/`

### 3. Run SQL Analytics & Index Simulation

```bash
python simulate_sql.py
```

This will:
- Query Parquet files using Spark SQL
- Output event/device analytics & simulated index queries

---

## ğŸ“Š Sample SQL Output

```text
+-------------+-------+
|device       |count  |
+-------------+-------+
|mobile       | 40212 |
|desktop      | 30014 |
|tablet       | 29774 |
+-------------+-------+

+----------+-------------+-------+
|event_type|device       |count  |
+----------+-------------+-------+
|login     |mobile       | 10045 |
|purchase  |desktop      | 10112 |
...
```

---

## ğŸ” Why This Project?

âœ… Designed for roles requiring:
- Data wrangling at scale
- PySpark transformations
- SQL-based analysis
- Scalable file formats (Parquet)
- Interview-readiness for real use cases

---

## ğŸ“§ Contact

Built by a transitioning **SAP Basis Engineer** learning **Data Engineering**.  
Feel free to connect for project collaboration or hiring:

- ğŸ’¼ GitHub: [your-username]
- ğŸ“§ Email: [your-email@example.com]
